{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPMogum4glBwYvYVxmX6sEI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Prerequisites before execution"],"metadata":{"id":"C-7tskvFyc6K"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MB5ykm3OrnVf"},"outputs":[],"source":["# Just in case of unmounted Drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# ...:: PARAMETERS : FILL THEN RUN ALL :::...\n","# ...:: PATH/NAME SECTION\n","\n","# Define the target directory path of the previous computed dataset\n","datasetDir = '/content/drive/My Drive/Colab Notebooks/datasets/'\n","# Define the dataset name used for this compute and fit\n","datasetName = 'multiple_256_tag_float32'\n","# Define the target directory path for saving the best val_mean_io_u weights during the fit\n","checkpointFilepath = '/content/drive/My Drive/Colab Notebooks/checkpoint/unet_model_multilabels_256_meaniou_best/'\n","# Define the target directory path for model saving\n","modelDir = '/content/drive/My Drive/Colab Notebooks/models/'\n","# Define the model name used for saving\n","modelName = 'unet_model_multilabels_256_meaniou_best.keras'\n","\n","\n","# ...:: MODEL PARAMETERS\n","\n","BATCH_SIZE = 32\n","BUFFER_SIZE = 1000\n","LEARNING_RATE = 0.0001\n","NUM_EPOCHS = 30\n","# VAL_SUBSPLITS is used in order to reduce validation steps. This is a way to reduce RAM used at each end epoch validation RAM uprising\n","VAL_SUBSPLITS = 5\n","\n","# ...:: MODEL EARLY STOPPING\n","# !!! WARNING : MONITOR VARIABLE NAMES ARE CORRECT FOR THE FIRST FIT ITERATION IN A SESSION, ON THE SECOND, val_mean_io_u will be named val_mean_io_u_1\n","early_stopping_loss = EarlyStopping(monitor='loss', patience=4, mode='min')\n","early_stopping_iou = EarlyStopping(monitor='mean_io_u', patience=4, mode='max')\n","early_stopping_val_loss = EarlyStopping(monitor='val_loss', patience=4, mode='min')\n","early_stopping_val_iou = EarlyStopping(monitor='val_mean_io_u', patience=6, mode='max')\n","model_checkpoint_val_iou = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpointFilepath,\n","    save_weights_only=True,\n","    monitor='val_mean_io_u',\n","    mode='max',\n","    save_best_only=True)"],"metadata":{"id":"hW5JB8bQxin2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Package Loading"],"metadata":{"id":"bQGaHT-Nysb1"}},{"cell_type":"code","source":["# ...:: NEEDED PACKAGE ::...\n","\n","import os\n","import pandas as pd\n","# import glob\n","# import shutil\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from time import time\n","\n","from PIL import Image, ImageFont, ImageDraw, ImageEnhance\n","import cv2\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","# from sklearn.preprocessing import LabelBinarizer\n","from sklearn.model_selection import train_test_split\n","\n","# import pickle\n","from tensorflow.keras.callbacks import EarlyStopping"],"metadata":{"id":"-dcE10DtwV50"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"e9fhkbHQwWCX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset Preparation"],"metadata":{"id":"38Oh3AoMzKz-"}},{"cell_type":"code","source":["# DATASET LOAD\n","dataset = tf.data.Dataset.load(os.path.join(datasetDir, datasetName))\n","\n","# DATASET PARTITIONING INTO TRAIN AND TEST\n","\n","# shuffle_size as half around of images number for a better split without surrounding RAM\n","def get_dataset_partitions_tf(ds, ds_size, train_split=0.75, test_split=0.25, shuffle=True, shuffle_size=2500):\n","    assert (train_split + test_split ) == 1\n","\n","    if shuffle:\n","        ds = ds.shuffle(shuffle_size, seed=32) # Specify seed to always have the same split distribution between runs\n","\n","    train_size = int(train_split * ds_size)\n","    train_ds = ds.take(train_size)\n","    test_ds = ds.skip(train_size)\n","\n","    return train_ds, test_ds\n","\n","train_dataset, test_dataset = get_dataset_partitions_tf(dataset, len(list(dataset)))"],"metadata":{"id":"AoX1rGqYwWL6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# FOR CONTROL ONLY\n","\n","# print(\"Len Train :\", len(train_dataset))\n","# print(\"Len Test :\", len(test_dataset))"],"metadata":{"id":"or1gc-8_wW4g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Build And Compile"],"metadata":{"id":"4lniMvOb1tRy"}},{"cell_type":"code","source":["# ... FONCTION SECTION ::...\n","\n","def double_conv_block(x, n_filters):\n","   # 2 Conv2D then ReLU activation\n","   x = layers.Conv2D(n_filters, 4, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(x)\n","   x = layers.Conv2D(n_filters, 4, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(x)\n","   return x\n","\n","def downsample_block(x, n_filters):\n","   f = double_conv_block(x, n_filters)\n","   p = layers.MaxPool2D(2)(f)\n","   p = layers.Dropout(0.3)(p)\n","   return f, p\n","\n","def upsample_block(x, conv_features, n_filters):\n","   # upsample\n","   x = layers.Conv2DTranspose(n_filters, 4, 2, padding=\"same\")(x)\n","   # concatenate\n","   x = layers.concatenate([x, conv_features])\n","   # dropout\n","   x = layers.Dropout(0.3)(x)\n","   # Conv2D twice with ReLU activation\n","   x = double_conv_block(x, n_filters)\n","   return x\n","\n","def build_unet_model():\n","   inputs = layers.Input(shape=(256,256,1))\n","\n","   # encoder: contracting path - downsample\n","   # 1 - downsample\n","   f1, p1 = downsample_block(inputs, 64)\n","   # 2 - downsample\n","   f2, p2 = downsample_block(p1, 128)\n","   # 3 - downsample\n","   f3, p3 = downsample_block(p2, 256)\n","   # 4 - downsample\n","   f4, p4 = downsample_block(p3, 512)\n","\n","   # 5 - bottleneck\n","   bottleneck = double_conv_block(p4, 1024)\n","\n","   # decoder: expanding path - upsample\n","   # 6 - upsample\n","   u6 = upsample_block(bottleneck, f4, 512)\n","   # 7 - upsample\n","   u7 = upsample_block(u6, f3, 256)\n","   # 8 - upsample\n","   u8 = upsample_block(u7, f2, 128)\n","   # 9 - upsample\n","   u9 = upsample_block(u8, f1, 64)\n","\n","   # outputs\n","   outputs = layers.Conv2D(4, (1, 1), padding=\"same\", activation = \"softmax\")(u9)\n","\n","   # unet model with Keras Functional API\n","   unet_model = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n","\n","   return unet_model\n","\n","@keras.saving.register_keras_serializable(package=\"MyLayers\")\n","class CustomLayer(keras.layers.Layer):\n","    def __init__(self, factor):\n","        super().__init__()\n","        self.factor = factor\n","\n","    def call(self, x):\n","        return x * self.factor\n","\n","    def get_config(self):\n","        return {\"factor\": self.factor}\n","\n","@keras.saving.register_keras_serializable(package=\"my_package\", name=\"dice_loss\")\n","def dice_loss(y_true, y_pred):\n","    # Convert y_true to float32 to match y_pred\n","    y_true = tf.cast(y_true, tf.float32)\n","    numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=-1)\n","    denominator = tf.reduce_sum(y_true + y_pred, axis=-1)\n","    return 1 - (numerator + 1) / (denominator + 1)\n","\n","# ... EXECUTE SECTION ::...\n","\n","# Adam optimizer function overloaded with learning rate\n","adamizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n","\n","\n","unet_model = build_unet_model()\n","\n","unet_model.compile(optimizer=adamizer,\n","                  loss=dice_loss,\n","                  metrics=[tf.keras.metrics.MeanIoU(num_classes=4)])"],"metadata":{"id":"KnJzP2IAwW_X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Fit"],"metadata":{"id":"_J7K9P103-u8"}},{"cell_type":"code","source":["start = time()\n","\n","TRAIN_LENGTH = len(train_dataset)\n","STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n","\n","TEST_LENGTH = int(len(test_dataset) - 1)\n","VALIDATION_STEPS = TEST_LENGTH // BATCH_SIZE // VAL_SUBSPLITS\n","\n","model_history = unet_model.fit(train_batches,\n","                              epochs=NUM_EPOCHS,\n","                              steps_per_epoch=STEPS_PER_EPOCH,\n","                              validation_steps=VALIDATION_STEPS,\n","                              validation_data=test_batches,\n","                              callbacks=[early_stopping_loss, early_stopping_iou, early_stopping_val_loss, early_stopping_val_iou, model_checkpoint_val_iou])\n","\n","# Elapsed fitting time display\n","print(\"Full fiting time :\", (time() - start) / 60, \" m\")"],"metadata":{"id":"PK5L_Oh1wXHf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Graph History Display"],"metadata":{"id":"OlXyIUYw4IMa"}},{"cell_type":"code","source":["# Plot creation with 2 subplots: (1 line, 2 columns)\n","fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n","\n","# Left Graph : Metric (Mean IoU in this case)\n","axs[0].plot(model_history.history['mean_io_u'], label='Train Mean IoU')\n","axs[0].plot(model_history.history['val_mean_io_u'], label='Validation Mean IoU')\n","axs[0].set_title('Model Mean IoU')\n","axs[0].set_ylabel('Mean IoU')\n","axs[0].set_xlabel('Epoch')\n","axs[0].legend(loc='lower right')\n","\n","# Right Graph : Loss (Dice Loss in this case)\n","axs[1].plot(model_history.history['loss'], label='Train Loss')\n","axs[1].plot(model_history.history['val_loss'], label='Validation Loss')\n","axs[1].set_title('Model Loss')\n","axs[1].set_ylabel('Loss')\n","axs[1].set_xlabel('Epoch')\n","axs[1].legend(loc='upper right')\n","\n","# Plots display\n","plt.show()"],"metadata":{"id":"sFBrt2y6wXOu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Saving"],"metadata":{"id":"ihqzHYyv4NSH"}},{"cell_type":"code","source":["unet_model.save(os.path.join(modelDir, modelName))"],"metadata":{"id":"a-PrTOyzwXWG"},"execution_count":null,"outputs":[]}]}